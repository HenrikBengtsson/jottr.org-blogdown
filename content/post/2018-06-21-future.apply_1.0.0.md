---
title: "future.apply 1.0.0 - Parallelize Any Base R Apply Function"
slug: "future.apply_1.0.0"
date: 2018-06-21
categories:
 - R
tags:
 - R
 - package
 - future
 - apply
 - lapply
 - mapply
 - sapply
 - tapply
 - vapply
 - Map
 - replicate
 - asynchronous
 - parallel processing
 - compute clusters
 - HPC
 
---

![0% to 100% utilization](/post/future.apply_1.0.0-htop_32cores.png)
_Got compute?_

[future.apply] 1.0.0 - _Apply Function to Elements in Parallel using Futures_ - is on CRAN.  With this milestone release, all<sup>*</sup> base R apply functions now have a corresponding futurized implementation.  This makes it easier than ever before to parallelize your existing `apply()`, `lapply()`, `mapply()`, ... code - just prepend `future_` to an apply call that takes a long time to complete. That's it! The default is sequential processing but, by using `plan(multiprocess)`, it'll run in parallel.

<br>
_Table: All future\_nnn() functions in the **future.apply** package.  Each function takes the same arguments as the corresponding **base** function does._<br>

Function                          | Description
----------------------------------|--------------------------------------
<code>future\_<strong>apply()</strong></code>  | Apply Functions Over Array Margins
<code>future\_<strong>lapply()</strong></code>    | Apply a Function over a List or Vector
<code>future\_<strong>sapply()</strong></code>    | - " -
<code>future\_<strong>vapply()</strong></code>    | - " -
<code>future\_<strong>replicate()</strong></code> | - " -
<code>future\_<strong>mapply()</strong></code>    | Apply a Function to Multiple List or Vector Arguments
<code>future\_<strong>Map()</strong></code>       | - " -
<code>future\_<strong>eapply()</strong></code> | Apply a Function Over Values in an Environment
<code>future\_<strong>tapply()</strong></code>    | Apply a Function Over a Ragged Array

<sup>*</sup> <code>future\_<strong>rapply()</strong></code> - Recursively Apply a Function to a List - is yet to be implemented.


## A Quick Example

```r
## Assume 100 samples each with 1,000 observation
X <- matrix(rnorm(500 * 100), ncol = 100)

## Wilcoxon Test for the first samples
fit <- wilcox.test(X[, 1], exact = TRUE, conf.int = TRUE)
print(fit)
# Wilcoxon signed rank test
# 
# data:  X[, 1]
# V = 259730, p-value = 0.2998
# alternative hypothesis: true location is not equal to 0
# 95 percent confidence interval:
#  -0.02921373  0.09536925
# sample estimates:
# (pseudo)median 
#     0.03270226 
```

```r
## Now lets apply the test for all 100 samples
## and measure the total processing time
t0 <- system.time({
  fits <- apply(X, MARGIN = 2,
                FUN = wilcox.test, exact = TRUE, conf.int = TRUE)
})
print(t0)
#    user  system elapsed 
#  52.250   0.045  52.320 
```

Let's try to parallelize using the future ecosystem.  This is done by replacing `apply()` with `future_apply()` in the code.  Then we'll benchmark what only this change will do when we still run sequentially.

```r
library(future.apply)

## Process sequentially (default if not specified)
plan(sequential)
t1 <- system.time({
  fits <- future_apply(X, MARGIN = 2,
                       FUN = wilcox.test, exact = TRUE, conf.int = TRUE)
})
print(t1 / t0)
#     user   system  elapsed 
# 1.034124 3.133333 1.036086 
```
Not too bad.  In this setup, the overhead cost from switching from using `apply()` to using a _sequential_ `future_apply()` is ~3%.


Next, let us try to process this in parallel on a machine with 32 cores.

```r
t32 <- system.time({
  fits <- future_apply(X, MARGIN = 2,
                       FUN = wilcox.test, exact = TRUE, conf.int = TRUE)
})
print(t32 / t0)
#      user     system    elapsed 
#       Inf        Inf 0.06416284 
```
That is a 16 times speedup - not too bad.  It's not 32 times faster because you always pay some overhead cost when you do parallel processing.  If we run the above benchmark test with 1, 2, ..., 32 workers;

```
ts <- sapply(1:32, FUN = function(ncores) {
  plan(multiprocess, workers = ncores)
  system.time({
    fits <- future_apply(X, MARGIN = 2,
                         FUN = wilcox.test, exact = TRUE, conf.int = TRUE)
  })
})
```

we can produce the following performance graph:
```r
stats <- data.frame(cores = 1:32, speedup = ts["elapsed", 1] / ts["elapsed", ])
ggplot(stats, aes(x = cores, y = speedup)) +
  geom_point(size = 2.0) +
  geom_smooth(method = "loess", se = FALSE, size = 1.5) +
  geom_abline(size = 1.5)
```

![Processing speedup as a function of the number of cores](/post/speedup_vs_cores.png)
_Figure: Processing speedup as a function of the number of cores. The black line represent the theoretical upper-bound speedup and the blue curve the observed speedup profile._


## Q&A

Q. _What are my options for parallelization?_<br>
A. Everything is processed through the [future] framework.  This means that all parallelization backends supported by the **parallel** package is supported out of the box, e.g. on your **local machine**, and on **local** or **remote** ad-hoc **compute clusters** (also in the **cloud**). Additional parallelization and distribution schemas are provided by backends such as **[future.callr]** (parallelization on your local machine) and **[future.batchtools]** (large-scale parallelization via **HPC job schedulers**).  For other alternatives, see the CRAN Page for the **[future]** package and the [High-Performance and Parallel Computing with R](https://cran.r-project.org/web/views/HighPerformanceComputing.html) CRAN Task View.

Q. _Righty-oh, so how do I specify what parallelization backend to use?_<br>
A. A fundamental design pattern of the future framework is that _the end user choose **how and where** to parallelize_ while _the developer choose **what** to parallelize_.  This means that you do _not_ specify the backend via some argument to the `future_nnn()` functions.  Instead, the backend is specified by the `plan()` function - you can almost think of it as a global option that the end user controls.  For example, `plan(multiprocess)` will parallelize on the local machine, so will `plan(future.callr::callr)`, whereas `plan(cluster, workers = c("n1", "n2", "remote.server.org"))` will parallelize on two local machines and one remote machine. Using `plan(future.batchtools::batchtools_sge)` will distribute the processing on you SGE-supported computer cluster.  BTW, you can also have [nested parallelization strategies](https://cran.r-project.org/web/packages/future/vignettes/future-3-topologies.html), e.g. `plan(list(tweak(cluster, workers = nodes), multiprocess))` where `nodes = c("n1", "n2", "remote.server.org")`.


Q. _What about load balancing?_<br>
A. The default behavior of all functions is to distribute **equally-sized chunks** of elements to each available background worker - such that each worker process exactly one chunk (= one future).  If the processing times various significantly across chunks, you can increase then average number of chunks processed by each worker, e.g. to have then process two chunks on average, specifying `future.scheduling = 2.0`.  Alternatively, you can specify the number of elements processed per chunk, e.g. `future.chunk.size = 10L` (an analogue to the `chunk.size` argument added to the **parallel** package in R 3.5.0).

Q. _What about random number generation (RNG)? I've heard it's tricky to get right when running in parallel._<br>
A. Just add `future.seed = TRUE` and you're good. This will use **parallel safe** and **statistical sound** **L'Ecuyer-CMRG RNG**, which is a well established parallel RNG algorithm.  The **future.apply** functions use this in a way that is also **invariant to** the future backend and the amount of "chunking" used.  Use `future.seed = 0xBEEF` to produce numerically reproducible results.

Q. _What about global variables? Whenever I've tried to parallelize code before, I often ran into errors on "this or that variable is missing"._<br>
A. This is very rarely a problem when using the [future] framework - things work out of the box.  **Global variables and packages** needed are **automatically identified** from static code inspection and passed on to the workers - even when the workers run on remote computers or in the cloud.


_Happy futuring!_



## Links
* future.apply package:
  - CRAN page: https://cran.r-project.org/package=future.apply
  - GitHub page: https://github.com/HenrikBengtsson/future.apply
* future package:
  - CRAN page: https://cran.r-project.org/package=future
  - GitHub page: https://github.com/HenrikBengtsson/future
* future.batchtools package:
  - CRAN page: https://cran.r-project.org/package=future.batchtools
  - GitHub page: https://github.com/HenrikBengtsson/future.batchtools
* doFuture package:
  - CRAN page: https://cran.r-project.org/package=doFuture
  - GitHub page: https://github.com/HenrikBengtsson/doFuture


## See also

* [Delayed Future(Slides from eRum 2018)](2018/06/18/future-erum2018-slides/), 2018-06-19
* [future 1.8.0: Preparing for a Shiny Future](/2018/04/12/future-results/), 2018-04-12
* [The Many-Faced Future](/2017/06/05/many-faced-future/), 2017-06-05
* [future 1.3.0 Reproducible RNGs, future&#95;lapply() and More](/2017/02/19/future-rng/), 2017-02-19
* [High-Performance Compute in R Using Futures](/2016/10/22/future-hpc/), 2016-10-22
* [Remote Processing Using Futures](/2016/10/11/future-remotes/), 2016-10-11

[future]: https://cran.r-project.org/package=future
[future.apply]: https://cran.r-project.org/package=future.apply
[future.batchtools]: https://cran.r-project.org/package=future.batchtools
[future.callr]: https://cran.r-project.org/package=future.callr
